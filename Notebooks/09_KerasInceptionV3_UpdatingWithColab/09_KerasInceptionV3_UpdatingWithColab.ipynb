{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import getpass\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os \n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adamax\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint    \n",
    "\n",
    "#!pip install keras==2.1.2 --upgrade\n",
    "# Basic Guides:\n",
    "# https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure setup\n",
    "batch_size = 40\n",
    "classes = 12\n",
    "N_training = 12000\n",
    "N_verification = 1000\n",
    "dropout = 0.4\n",
    "kernel_reg = 0.2\n",
    "dense_size = 2048\n",
    "train_layers = 229 #179 #229 #249\n",
    "learning_rate = 1e-2\n",
    "learning_rate_min = 1e-9\n",
    "base_path = 'C:/Data/Workspace/GitHub/DeepLearningProject/LocalContent/Logs/' \n",
    "custom_path = 'Run_v212_400_v1_TestRegFrom_v4'\n",
    "\n",
    "# Create path\n",
    "main_path = base_path + custom_path\n",
    "\n",
    "# Create filename\n",
    "filename = 'Par_T' + str(N_training) + '_V' + str(N_verification) + '_D' \\\n",
    "+ str(dense_size) + '_L' + str(train_layers) +'_B' + str(batch_size) + '_Drop' \\\n",
    "+ str(dropout).replace('.','_') + '_KReg' + str(kernel_reg).replace('.','_') \\\n",
    "+ '_Lr' + str(learning_rate).replace('.','_') + '_minLr' + str(learning_rate_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_plants(source):\n",
    "    import shutil\n",
    "    \n",
    "    source1 = \"C:/Data/Workspace/GitHub/DeepLearningProject/LocalContent/Datasets/Resized_400/train\"\n",
    "    dest11 =  \"C:/Data/Workspace/GitHub/DeepLearningProject/LocalContent/Datasets/Resized_400/verf\"\n",
    "    folders = os.listdir(source)\n",
    "\n",
    "    x_data = []\n",
    "    \n",
    "    for f in folders:\n",
    "        files = os.listdir(source + '/'+ f)\n",
    "        for i in files:\n",
    "            x = image.load_img(source + '/'+ f + '/' + i)\n",
    "            x = img_to_array(x)\n",
    "            x_data.append(x)\n",
    "    return x_data\n",
    "\n",
    "# Class for modifying the tensorboard plotting\n",
    "class TrainValTensorBoard(keras.callbacks.TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')        \n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "      \n",
    "        # Adding Learning rate to the logs\n",
    "#         ittr = (self.model.optimizer.iterations)\n",
    "#         decay = (self.model.optimizer.decay)\n",
    "#         lr = (self.model.optimizer.lr)\n",
    "#         beta_1 = (self.model.optimizer.beta_1)\n",
    "        \n",
    "#         lr *= (1. / (1. + decay * K.cast(ittr, K.dtype(decay))))\n",
    "#         t = K.cast(ittr, K.floatx()) + 1\n",
    "#         lr_t = lr / (1. - K.pow(beta_1, t))\n",
    "        lr = (self.model.optimizer.lr)\n",
    "        lr_t = lr\n",
    "        \n",
    "        \n",
    "        logs['learning_rate'] = np.float64(K.eval(lr_t))\n",
    "        logs['val_learning_rate'] = np.float64(K.eval(lr_t))\n",
    "           \n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3783 images belonging to 12 classes.\n",
      "Found 967 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Configure the loading of data\n",
    "# Image Generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=360,\n",
    "    #shear_range=0.3,\n",
    "    zoom_range = 0.5,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"../../LocalContent/Datasets/Resized_400/train\",  # this is the target directory\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical') \n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        \"../../LocalContent/Datasets/Resized_400/verf\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Configure model\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a dropout layer\n",
    "x = Dropout(rate = dropout)(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(dense_size, activation='relu',kernel_regularizer=regularizers.l2(kernel_reg))(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "300/300 [==============================] - 431s 1s/step - loss: 3.2129 - val_loss: 2.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1844493d128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for the top layers\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done*after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# https://stackoverflow.com/questions/45943675/meaning-of-validation-steps-in-keras-sequential-fit-generator-parameter-list\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=N_training//batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=N_verification//batch_size,\n",
    "        max_queue_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load possible weights for the model\n",
    "model.load_weights('C:/Data/Google Drive/DeepLearningDataStorage/Run_v212_299_v1_TestRegFrom_v4/Par_T7000_V1000_D2048_L229_B64_Drop0_4_KReg0_2_Lr0_01_minLr1e-09.E18-L0.37-A0.93.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Epoch 1/500\n",
      "300/300 [==============================] - 593s 2s/step - loss: 1.5447 - acc: 0.5465 - val_loss: 0.6884 - val_acc: 0.8294\n",
      "Epoch 2/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.8035 - acc: 0.7954 - val_loss: 0.5255 - val_acc: 0.8873\n",
      "Epoch 3/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.6885 - acc: 0.8302 - val_loss: 0.4940 - val_acc: 0.8883\n",
      "Epoch 4/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.6295 - acc: 0.8488 - val_loss: 0.4825 - val_acc: 0.9007\n",
      "Epoch 5/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.5920 - acc: 0.8586 - val_loss: 0.3924 - val_acc: 0.9173\n",
      "Epoch 6/500\n",
      "300/300 [==============================] - 567s 2s/step - loss: 0.5741 - acc: 0.8638 - val_loss: 0.3804 - val_acc: 0.9338\n",
      "Epoch 7/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.5478 - acc: 0.8728 - val_loss: 0.3452 - val_acc: 0.9411\n",
      "Epoch 8/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.4939 - acc: 0.8840 - val_loss: 0.4093 - val_acc: 0.9059\n",
      "Epoch 9/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.5070 - acc: 0.8821 - val_loss: 0.4445 - val_acc: 0.8945\n",
      "Epoch 10/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.4867 - acc: 0.8912 - val_loss: 0.3986 - val_acc: 0.9162\n",
      "Epoch 11/500\n",
      "299/300 [============================>.] - ETA: 1s - loss: 0.4876 - acc: 0.8902\n",
      "Epoch 00011: reducing learning rate to 0.007499999832361937.\n",
      "300/300 [==============================] - 567s 2s/step - loss: 0.4870 - acc: 0.8905 - val_loss: 0.3885 - val_acc: 0.9173\n",
      "Epoch 12/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.4313 - acc: 0.8992 - val_loss: 0.3078 - val_acc: 0.9380\n",
      "Epoch 13/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.4088 - acc: 0.9069 - val_loss: 0.3147 - val_acc: 0.9307\n",
      "Epoch 14/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3843 - acc: 0.9093 - val_loss: 0.3521 - val_acc: 0.9152\n",
      "Epoch 15/500\n",
      "300/300 [==============================] - 566s 2s/step - loss: 0.3592 - acc: 0.9144 - val_loss: 0.3269 - val_acc: 0.9173\n",
      "Epoch 16/500\n",
      "299/300 [============================>.] - ETA: 1s - loss: 0.3713 - acc: 0.9146\n",
      "Epoch 00016: reducing learning rate to 0.005624999874271452.\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3709 - acc: 0.9148 - val_loss: 0.3087 - val_acc: 0.9297\n",
      "Epoch 17/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3572 - acc: 0.9154 - val_loss: 0.2665 - val_acc: 0.9462\n",
      "Epoch 18/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3380 - acc: 0.9225 - val_loss: 0.2901 - val_acc: 0.9442\n",
      "Epoch 19/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3143 - acc: 0.9268 - val_loss: 0.2700 - val_acc: 0.9390\n",
      "Epoch 20/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3340 - acc: 0.9239 - val_loss: 0.2855 - val_acc: 0.9431\n",
      "Epoch 21/500\n",
      "299/300 [============================>.] - ETA: 1s - loss: 0.3303 - acc: 0.9242\n",
      "Epoch 00021: reducing learning rate to 0.004218749818392098.\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.3299 - acc: 0.9244 - val_loss: 0.2941 - val_acc: 0.9297\n",
      "Epoch 22/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.2917 - acc: 0.9320 - val_loss: 0.2572 - val_acc: 0.9380\n",
      "Epoch 23/500\n",
      "300/300 [==============================] - 567s 2s/step - loss: 0.2854 - acc: 0.9331 - val_loss: 0.2644 - val_acc: 0.9359\n",
      "Epoch 24/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.2801 - acc: 0.9342 - val_loss: 0.2245 - val_acc: 0.9483\n",
      "Epoch 25/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.2734 - acc: 0.9373 - val_loss: 0.2487 - val_acc: 0.9462\n",
      "Epoch 26/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.2751 - acc: 0.9354 - val_loss: 0.2286 - val_acc: 0.9524\n",
      "Epoch 27/500\n",
      "300/300 [==============================] - 566s 2s/step - loss: 0.2765 - acc: 0.9371 - val_loss: 0.2317 - val_acc: 0.9504\n",
      "Epoch 28/500\n",
      "300/300 [==============================] - 564s 2s/step - loss: 0.2700 - acc: 0.9382 - val_loss: 0.2164 - val_acc: 0.9566\n",
      "Epoch 29/500\n",
      "300/300 [==============================] - 565s 2s/step - loss: 0.2614 - acc: 0.9362 - val_loss: 0.2419 - val_acc: 0.9462\n",
      "Epoch 30/500\n",
      "300/300 [==============================] - 566s 2s/step - loss: 0.2641 - acc: 0.9366 - val_loss: 0.2862 - val_acc: 0.9462\n",
      "Epoch 31/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.2783 - acc: 0.9338 - val_loss: 0.2566 - val_acc: 0.9483\n",
      "Epoch 32/500\n",
      "299/300 [============================>.] - ETA: 1s - loss: 0.2629 - acc: 0.9414\n",
      "Epoch 00032: reducing learning rate to 0.003164062276482582.\n",
      "300/300 [==============================] - 562s 2s/step - loss: 0.2625 - acc: 0.9415 - val_loss: 0.2541 - val_acc: 0.9473\n",
      "Epoch 33/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.2350 - acc: 0.9449 - val_loss: 0.2283 - val_acc: 0.9483\n",
      "Epoch 34/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.2378 - acc: 0.9463 - val_loss: 0.2371 - val_acc: 0.9514\n",
      "Epoch 35/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.2381 - acc: 0.9453 - val_loss: 0.2314 - val_acc: 0.9411\n",
      "Epoch 36/500\n",
      "299/300 [============================>.] - ETA: 1s - loss: 0.2207 - acc: 0.9485\n",
      "Epoch 00036: reducing learning rate to 0.0023730467073619366.\n",
      "300/300 [==============================] - 562s 2s/step - loss: 0.2205 - acc: 0.9485 - val_loss: 0.2460 - val_acc: 0.9421\n",
      "Epoch 37/500\n",
      "300/300 [==============================] - 563s 2s/step - loss: 0.2241 - acc: 0.9453 - val_loss: 0.2107 - val_acc: 0.9493\n",
      "Epoch 38/500\n",
      "300/300 [==============================] - 560s 2s/step - loss: 0.2075 - acc: 0.9525 - val_loss: 0.2073 - val_acc: 0.9493\n",
      "Epoch 39/500\n",
      "300/300 [==============================] - 562s 2s/step - loss: 0.2128 - acc: 0.9512 - val_loss: 0.2195 - val_acc: 0.9462\n",
      "Epoch 40/500\n",
      "300/300 [==============================] - 612s 2s/step - loss: 0.2155 - acc: 0.9484 - val_loss: 0.2097 - val_acc: 0.9504\n",
      "Epoch 41/500\n",
      "  8/300 [..............................] - ETA: 8:30 - loss: 0.1900 - acc: 0.9625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-582058298178>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_verification\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_checker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         max_queue_size=5)#, workers=2, use_multiprocessing=True) #initial_epoch=76\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\conda\\conda\\envs\\tensorflowv3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Part\n",
    "for layer in model.layers[:train_layers]: \n",
    "   layer.trainable = False\n",
    "for layer in model.layers[train_layers:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=SGD(lr=learning_rate, momentum=0.9, nesterov = True), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=Adamax(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=K.epsilon()), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75,patience=3, min_lr=learning_rate_min,cooldown=1,verbose=1)\n",
    "model_checker = ModelCheckpoint(main_path + '/' + filename + '.E{epoch:02d}-L{val_loss:.3f}-A{val_acc:.3f}.hdf5', \n",
    "                                monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "#history = TrainValTensorBoard(log_dir=main_path + \"/{}\".format(time()), histogram_freq=0, write_graph=False, write_images=False)\n",
    "history = TrainValTensorBoard(log_dir=main_path + '/' + filename, histogram_freq=0, write_graph=False, write_images=False)\n",
    "#history = keras.callbacks.TensorBoard(log_dir=\"tmp/logs/{}\".format(time()), histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir=\"../../LocalContent/Logs/08_KerasInceptionV3_TestingDirectionLoading/{}\".format(time()), histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=N_training//batch_size,\n",
    "        epochs=500,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=N_verification//batch_size,\n",
    "        callbacks=[history,reduce_lr,model_checker],\n",
    "        max_queue_size=5)#, workers=2, use_multiprocessing=True) #initial_epoch=76\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(main_path + '/Lastest.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
